## This is the replication package for the paper "Assessing Programming Task Difficulty for Efficient Evaluation of Large Language Models".

A preprint is available [here](). This is a work in progress.

## Organization of the repository

* `data/`: contains all data from the experiments.
  * `humaneval/` and `classeval/` are the data on the benchmarks.  Each directory contains the prompts generated for all tasks (`prompts_generated_X.csv`) as well as the code generated by the Code LLMs (`raw/`). It also contains the functional correctness assessment (`post_test/`), syntactic correctness (`sim/`) and AST nodes extraced (`structure_types_X.json`)

  * `prompt_templates/` contains the templates we used to generate the prompts as well as to generate the new tasks.

  * `irt_data/` contains the difficulty, discriminant and ability as calculated by the IRT models.

  * `humanexp/` contains the results of the human study.

* `scripts/`: contains all the scripts to run DiffEval process. The usage of each scripts is explained below.

## Setting up the environment

We used Python 3.10 in our experiments in an anaconda environments. The file `environment.yaml` can be used to recreate the environment with anaconda (`conda env create -f environment.yml`).

The scripts for having GPT4 generates the prompts as well as having the LLMs generate codes uses OpenAI and HuggingFace API. As such, both an OpenAI key and HuggingFace token are required. For the scripts to work, one needs to add a `key.json` file in the `script/` directory. This file needs to contain both OpenAI and HuggingFace key/token as follow:

```json
{'OPENAI': YOUR_OPENAI_KEY,
'HF_TOKEN': YOUR_HF_TOKEN}
```

## Results of the RQs

To replicate the results of our RQs, we provide a jupyter notebook `RQ1_to_RQ4.ipynb` that encompasses all our experiments presented in the paper. Each RQ can be executed independently. No pre-processing is required as all the data are available in the repository. Below we provide a step-by-step procedure to obtain the different data necessary to run `RQ1_to_RQ4.ipynb` if you wish to replicate our results.

## Generating the level/rephrasing prompts

The script used is `generate_prompts.py`. It generate either the level type prompts or the rephrase type prompts on a given dataset or on the new generated tasks (sub_benchmark).

Usage:

```python
generate_prompts.py [-h] [-d DATASET] [-s SUB_BENCHMARK] [--levels] [--rephrase]

options:
  -h, --help            show this help message and exit
  -d DATASET, --dataset DATASET [humaneval|ClassEval]
  --levels
  --rephrase
```

As the prompts are generated first on the levels and then on the rephrasing, one should first run the script with the `--levels` flag and then with the `--rephrase` flag.

The generated output will be a `.json` file named `prompts_generated_{DATASET}.csv`. In the provided script, the files are saved at the root of the `script/` folder. The actual data are in `data/`. 

## Running on the code LLMs

The scripts used are `run_llm.py` and `run_llm_greedy.py`. The scripts let a model generates codes for based on the prompts of a given dataset/sub_benchmark. 

Usage:

```python
run_llm.py [-h] [-m MODEL] [-d DATASET] [-s SUB_BENCHMARK]

options:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL [llama|gemma|gpt|deepseek|magicoder]
  -d DATASET, --dataset DATASET [humaneval|ClassEval]
```

The generated output will be a `.json` file named `results_{DATASET}_{MODEL}.json` for the script `run_llm.py`. For `run_greedy_llm.py` will have the same output with a `_greedy.json` at the end. In the provided scripts, the files are saved at the root of the `script/` folder. The actual data are in `data/`. 

The outputs of this script should be stored in the `raw/` sub-folder of each data folder. For instance, `results_humanevalplus_gpt.json` should be placed in `data/humanevalplus/raw/`

## Checking the functional correctness

To assess the code generated, we do the following:

* For HumanEval+, we make use of the test pipeline provided by the official implementation [EvalPlus](https://github.com/evalplus/evalplus). The final output should be a `.json` file structured with key related to HumanEval task_id. Inside each key, a nested dictionary is contained with the three levels which themselves contain a list of tuple (code, code_result).

```json
{
'0': {'level 1': [(SOME_CODE, TRUE/FALSE), ..., (SOME_CODE, TRUE/FALSE)],
      'level 2': [(SOME_CODE, TRUE/FALSE), ..., (SOME_CODE, TRUE/FALSE)],
      'level 3': [(SOME_CODE, TRUE/FALSE), ..., (SOME_CODE, TRUE/FALSE)]},
'1': { ...}
}
```
This script is used both for greedy and normal approach. 

* For ClassEval, we will use a modified version of their pipeline. To execute it, run the `evaluation.py` in `script/test_pipelines/`. The only parameter to modify is the source model (for instance `-so gpt`).

```python
evaluation.py [-h] [-so SOURCE_MODEL] [-e EVAL_DATA] [-st SAMPLED_TASKS]

options:
  -h, --help            show this help message and exit
  -so SOURCE_MODEL, --source_model SOURCE_MODEL
                        name of the model to evaluate
  -e EVAL_DATA, --eval_data EVAL_DATA
                        ClassEval data
  -st SAMPLED_TASKS, --sampled_tasks SAMPLED_TASKS
                        sampled_tasks
```
This script is used both for greedy and normal approach. For the greedy, one should add `_greedy` to the model name, e.g., `-so gpt_greedy`.

*Note*: Some requirements are needed to run the tasks of the benchmarks. One should install the requirements using the `rq.txt` file in each of the `data` repository. We recommend to use a Docker installation to run those pipelines.

The results obtained are `.json` files named `results_{DATASET}_{MODEL}_eval.json`. They are all formatted similarly to the example in the HumanEval+ pipeline. All the results obtain from those scripts will be saved in a `post_test/` sub-directory in each of the data sub-folders. For instance, `results_humanevalplus_gpt_eval.json` will be in `data/humanevalplus/post_test/`

## Getting the similarity

The next step is to compute the similarity using the CodeBLEU metric. To do so, depending on the benchmark, one should use the correct script from the `script/sim` folder. They all follow the same approach.

The results obtained are `.json` files named `results_{DATASET}_{MODEL}_sim.json`. All the results obtain from those scripts will be saved in a `sim/` sub-directory in each of the data sub-folders. For instance, `results_humanevalplus_gpt_sim.json` will be in `data/humanevalplus/sim/`

## Training the IRT model

The code for the IRT model is in `irt.py`. We use a modification of the Beta-3 IRT model from the [`birt-gd`](https://github.com/Manuelfjr/birt-gd/tree/main): the modifications boil down to adopting a similar initialization in the Beta-3 model as to what is used in the Beta-4 model, which improve the performance without needing to split the discriminant into two sub-variables. 

```python

irt.py [-h] [-d DATASET]

options:
  -h, --help            show this help message and exit
  -d DATASET, --dataset DATASET
```

where `DATASET` is either `humanevalplus` or `ClassEval`. This will create three files in the relevant `data/DATASET/irt_data` directory, one for the difficulty, one for the discriminant and one for the ability.

To verify the modified algorithm works correctly, we can do a sanity check by reproducing the results from the toy dataset of the original [Beta-3 IRT model](https://proceedings.mlr.press/v89/chen19b.html): by running the script `irt_test.py`, we end up with a similar result to Figure 4 of the paper, that is a higher difficulty/lower discriminant along the classification boundary compared to edges, as well as a higher difficulty/lower discriminant on noisy data points that are wrongly classified.

## Getting the AST nodes

In RQ3, we need to extract the AST nodes for relevant codes to cross compare them to difficulty/discriminant of the tasks. To do so, we use the similarity as calculated previously to prune the code snippets with a similarity < 0.5 compared to a correct (i.e. that passes all the test). As we have access to the oracle, we always have one correct code to compare to. This way, we end up with relevant codes that we extract AST nodes from. One should run:

```python
ast_tasks_check.py [-h] [-d DATASET]

options:
  -h, --help            show this help message and exit
  -d DATASET, --dataset DATASET
```

where `DATASET` is either `humanevalplus` or `ClassEval`. This will create a `structure_types_X.json` in the corresponding `data/` repository.

